生成对抗网络GAN
=============
# 一.GAN基本思想
> 生成式对抗网络GAN (Generative adversarial networks) 是Goodfellow 等在2014年提出的一种生成式模型。GAN 的核心思想来源于博弈论的纳什均衡。它设定参与游戏双方分别为一个生成器(Generator)和一个判别器(Discriminator), 生成器捕捉真实数据样本的潜在分布, 并生成新的数据样本; 判别器是一个二分类器, 判别输入是真实数据还是生成的样本。 
为了取得游戏胜利, 这两个游戏参与者需要不断优化, 各自提高自己的生成能力和判别能力, 这个学习优化过程就是寻找二者之间的一个纳什均衡。生成器和判别器均可以采用目前研究火热的深度神经网络.


# 二.GAN结构
> GAN的计算流程与结构如图：
![Image text](https://github.com/ShaoQiBNU/Generative_Adversarial_Nets/blob/master/images/1.png)

> 任意可微分的函数都可以用来表示GAN 的生成器和判别器, 由此,我们用可微分函数D和G来分别表示判别器和生成器, 它们的输入分别为真实数据x 和随机变量z。G(z)为由G生成的尽量服从真实数据分布pdata的样本。如果判别器的输入来自真实数据, 标注为1.如果输入样本为G(z), 标注为0。 
这里D 的目标是实现对数据来源的二分类判别: 真(来源于真实数据x的分布) 或者伪(来源于生成器的伪数据G(z)),而G的目标是使自己生成的伪数据G(z)在D上的表现D(G(z))和真实数据x在D上的表现D(x)一致.

# 三. GAN训练方法

> 判别模型： 希望真样本集尽可能输出1，假样本集输出0。对于判别网络，此时问题转换成一个有监督的二分类问题，直接送到神经网络模型中训练就ok。

> 生成网络：目的是生成尽可能逼真的样本。原始生成网络如何知道真不真？就是送到判别网络中。在训练生成网络的时候，需要联合判别网络才能达到训练的目的。什么意思？如果单单只用生成网络，那么后面怎么训练，误差来源在哪里？细想一下没有，但是如果把刚才的判别网络串接在生成网络的后面，这样就知道真假来，也就有了误差！！！所以对生成网络的训练其实是对生成-判别网络串接的训练，就像图中显示的那样。生成假样本，要把这些假样本的标签都设置为 1，也就是认为这些假样本在生成网络训练的时候是真样本。起到了迷惑判别器的目的！！！在训练这个串接网络的时候，一个很重要的操作就是不要判别网络的参数发生变化，也就是不让它参数发生更新，只是把误差一直传，传到生成网络那块后更新生成网络的参数，这样就完成来生成网络的训练。

> 论文里的公式如下：
![Image text](https://github.com/ShaoQiBNU/Generative_Adversarial_Nets/blob/master/images/2.png)

> 简单分析一下这个公式，整个式子由两项构成。x表示真实图片，z表示输入G网络的噪声，而G(z)表示G网络生成的图片。
D(x)表示D网络判断真实图片是否真实的概率（因为x就是真实的，所以对于D来说，这个值越接近1越好）。
D(G(z))是D网络判断G生成的图片是否真实的概率，G应该希望自己生成的图片“越接近真实越好”。也就是说，G希望D(G(z))尽可能得大，这时V(D, G)会变小。因此式子的最前面的记号是min_G。

> D的目的：D的能力越强，D(x)应该越大，D(G(x))应该越小。这时V(D,G)会变大。因此式子对于D来说是求最大(max_D)
G的目的：G的能力越强，D(G(z))应该越大，D(x)应该越小。这时V(D,G)会变小。因此式子对于G来说是求最小(min_G)

> 训练两个模型的方法：单独交替迭代训练，算法过程如下：
![Image text](https://github.com/ShaoQiBNU/Generative_Adversarial_Nets/blob/master/images/3.png)
这里红框圈出的部分是我们要额外注意的。第一步我们训练D，D是希望V(G, D)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(G, D)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。

> 优化D，即优化判别网络时，没有生成网络什么事，后面的G(z)这里就相当于已经得到的假样本。优化D的公式的第一项，使得真样本x输入的时候，得到的结果越大越好，因为真样本的预测结果越接近1越好；对于假样本，需要优化的是其结果越小越好，也就是D(G(z))越小越好，因为它的标签为0。但是第一项越大，第二项越小，就矛盾了，所以把第二项改为1-D(G(z))，这样就是越大越好。
![Image text](https://github.com/ShaoQiBNU/Generative_Adversarial_Nets/blob/master/images/4.png)

> 优化G的时候，这个时候没有真样本什么事，所以把第一项直接去掉，这时候只有假样本，但是我们说这个时候是希望假样本的标签是1，所以是D(G(z))越大越好，但是为了统一成1-D(G(z))的形式，那么只能是最小化1-D(G(z))，本质上没有区别，只是为了形式的统一。之后这两个优化模型可以合并起来写，就变成最开始的最大最小目标函数了。
![Image text](https://github.com/ShaoQiBNU/Generative_Adversarial_Nets/blob/master/images/5.png)





